{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "22gli6ZkIKDj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO7sS9VrrVZF",
        "outputId": "a98a5393-df89-42fb-ee88-d7201dfe5ffd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_1WBdYCBIdG3"
      },
      "outputs": [],
      "source": [
        "class PreContextRewardModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "    super(PreContextRewardModel, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Embedding layers for human and machine pre-context\n",
        "    self.embedding_human = nn.Embedding(input_size, hidden_size)\n",
        "    self.embedding_machine = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "    # GRU layer\n",
        "    self.gru = nn.GRU(hidden_size * 2, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    # Output layer\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x_human, x_machine, hidden):\n",
        "    # Embed the input tokens for both human and machine part\n",
        "    x_human = self.embedding_human(x_human)\n",
        "    x_machine = self.embedding_machine(x_machine)\n",
        "\n",
        "    # Concatenate the embeddings\n",
        "    x = torch.cat((x_human, x_machine), dim=2)\n",
        "\n",
        "    # Pass through GRU\n",
        "    out, hidden = self.gru(x, hidden)\n",
        "\n",
        "    # Take the output of the last time step\n",
        "    out = out[:, -1, :]\n",
        "\n",
        "    # Pass through fully connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    # out = self.softmax(out)\n",
        "\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpWwoN0cwjFB",
        "outputId": "217a4032-5744-4d8f-dff9-e919d30d378c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cFfGTCtYw2on"
      },
      "outputs": [],
      "source": [
        "import json, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oZhKfk35xmhA"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/ECE570/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qkqeUOC0ySxN"
      },
      "outputs": [],
      "source": [
        "def create_duet_pairs(parts):\n",
        "  \"\"\"Create duet pairs from the four parts.\"\"\"\n",
        "  duet_pairs = [\n",
        "    (parts[\"soprano\"], parts[\"alto\"]),\n",
        "    (parts[\"soprano\"], parts[\"tenor\"]),\n",
        "    (parts[\"soprano\"], parts[\"bass\"]),\n",
        "    (parts[\"alto\"], parts[\"tenor\"]),\n",
        "    (parts[\"alto\"], parts[\"bass\"]),\n",
        "    (parts[\"tenor\"], parts[\"bass\"]),\n",
        "  ]\n",
        "  return duet_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xa21XTl8yo3p"
      },
      "outputs": [],
      "source": [
        "def tokenize_part(part):\n",
        "  \"\"\"Convert a part into a sequence of tokenized notes.\"\"\"\n",
        "  tokens = []\n",
        "  for n in part:\n",
        "    if n == \"hold\":\n",
        "      tokens.append(128)\n",
        "    else:\n",
        "      tokens.append(n)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yLZo9Oiwy3Rf"
      },
      "outputs": [],
      "source": [
        "duet_data = []\n",
        "for filename in os.listdir(dataset_path):\n",
        "  if filename.endswith(\".json\"):  # Check if the file is a JSON file\n",
        "    filepath = os.path.join(dataset_path, filename)\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "      chorale_data = json.load(file)\n",
        "      duet_pairs = create_duet_pairs(chorale_data)\n",
        "      for human_part, machine_part in duet_pairs:\n",
        "        human_tokens = tokenize_part(human_part)\n",
        "        machine_tokens = tokenize_part(machine_part)\n",
        "        duet_data.append((human_tokens, machine_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0xqI_BHa0bXs"
      },
      "outputs": [],
      "source": [
        "def prepare_training_data(duet_data, window_size=16):\n",
        "  \"\"\"Prepare training data for the reward model.\"\"\"\n",
        "  inputs_human = []\n",
        "  inputs_machine = []\n",
        "  targets = []\n",
        "  for human_tokens, machine_tokens in duet_data:\n",
        "    # Ensure both parts have the same length\n",
        "    min_length = min(len(human_tokens), len(machine_tokens))\n",
        "    human_tokens = human_tokens[:min_length]\n",
        "    machine_tokens = machine_tokens[:min_length]\n",
        "    # Create input-target pairs\n",
        "    for i in range(window_size, min_length):\n",
        "      # input_seq = [human_tokens[i-window_size:i], machine_tokens[i-window_size:i]]\n",
        "      inputs_human.append(human_tokens[i-window_size:i])\n",
        "      inputs_machine.append(machine_tokens[i-window_size:i])\n",
        "      targets.append(machine_tokens[i])\n",
        "\n",
        "  return inputs_human, inputs_machine, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZGe2Khr218bv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare dataset\n",
        "inputs_human, inputs_machine, targets = prepare_training_data(duet_data)\n",
        "\n",
        "# Split into train and test sets (e.g., 80% train, 20% test)\n",
        "# train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
        "#     inputs, targets, test_size=0.2, random_state=42\n",
        "# )\n",
        "(\n",
        "    train_inputs_human, test_inputs_human,\n",
        "    train_inputs_machine, test_inputs_machine,\n",
        "    train_targets, test_targets\n",
        ") = train_test_split(inputs_human, inputs_machine, targets, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aAECEuS41_Y3"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "input_size = 129  # Number of unique tokens (MIDI pitches + rest)\n",
        "hidden_size = 256\n",
        "output_size = 129  # Same as input_size\n",
        "num_layers = 2\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 10\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MEJJewE-2rE5"
      },
      "outputs": [],
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "model = PreContextRewardModel(input_size, hidden_size, output_size, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs_human = torch.tensor(train_inputs_human, dtype=torch.long).to(device)\n",
        "train_inputs_machine = torch.tensor(train_inputs_machine, dtype=torch.long).to(device)\n",
        "train_targets = torch.tensor(train_targets, dtype=torch.long).to(device)"
      ],
      "metadata": {
        "id": "8yR1Pu0o1cor"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHhugQ8o24WV",
        "outputId": "6278b0c1-0b23-4319-d8bc-5978da90efd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6050\n",
            "Epoch [2/10], Loss: 0.5764\n",
            "Epoch [3/10], Loss: 0.4232\n",
            "Epoch [4/10], Loss: 0.4290\n",
            "Epoch [5/10], Loss: 0.5742\n",
            "Epoch [6/10], Loss: 0.4625\n",
            "Epoch [7/10], Loss: 0.3621\n",
            "Epoch [8/10], Loss: 0.5798\n",
            "Epoch [9/10], Loss: 0.3163\n",
            "Epoch [10/10], Loss: 0.2485\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "\n",
        "  for i in range(0, len(train_inputs_human), batch_size):\n",
        "    # Get batch\n",
        "    batch_inputs_human = train_inputs_human[i:i+batch_size]\n",
        "    batch_inputs_machine = train_inputs_machine[i:i+batch_size]\n",
        "    batch_targets = train_targets[i:i+batch_size]\n",
        "\n",
        "    # Dynamically initialize hidden state\n",
        "    hidden = model.init_hidden(len(batch_inputs_human))\n",
        "\n",
        "    # Forward pass\n",
        "    hidden = hidden.detach()  # Detach hidden state to avoid backprop through time\n",
        "    train_outputs, hidden = model(batch_inputs_human, batch_inputs_machine, hidden)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(train_outputs, batch_targets)\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Apply gradient clipping (since GRU gradients can explode)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Y9OBwnDqKR5j"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/ECE570/models/pre_context_reward_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs_human = torch.tensor(test_inputs_human, dtype=torch.long).to(device)\n",
        "test_inputs_machine = torch.tensor(test_inputs_machine, dtype=torch.long).to(device)\n",
        "test_targets = torch.tensor(test_targets, dtype=torch.long).to(device)"
      ],
      "metadata": {
        "id": "MyFpfmqa4b6g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_inputs_human, test_inputs_machine, test_targets):\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        hidden = model.init_hidden(len(test_inputs_human))\n",
        "        test_outputs, _ = model(test_inputs_human, test_inputs_machine, hidden)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(test_outputs, test_targets)\n",
        "    print(f\"Test Loss: {loss.item()}\")\n",
        "\n",
        "    # Convert outputs to probabilities\n",
        "    probs = torch.softmax(test_outputs, dim=1)\n",
        "    predicted = torch.argmax(probs, dim=1)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = (predicted == test_targets).float().mean().item()\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_model(model, test_inputs_human, test_inputs_machine, test_targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGNE9TClvDy-",
        "outputId": "944d5bb5-072c-470d-eda3-2287fcce437d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6871500015258789\n",
            "Test Accuracy: 79.07%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}